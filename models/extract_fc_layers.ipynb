{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the main external libs we'll use\n",
    "import fastai\n",
    "from fastai.imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch \n",
    "import numpy \n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "from fastai import conv_learner\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for respective architectures\n",
    "arch1 = resnet50\n",
    "PATH1 = \"../../CHRC_NEW_DATA/chrc_data_patches_norm_1000_dup/\"\n",
    "sz1=256\n",
    "\n",
    "\n",
    "arch2=resnet50\n",
    "PATH2 = \"../../CHRC_NEW_DATA/chrc_data_patches_norm_duplicated/\"\n",
    "sz2=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = ImageClassifierData.from_paths(PATH1, tfms=tfms_from_model(arch1, sz1))\n",
    "learn1 = ConvLearner.pretrained(arch1, data1, precompute=False)\n",
    "learn1.precompute = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = ImageClassifierData.from_paths(PATH2, tfms=tfms_from_model(arch2, sz2))\n",
    "learn2 = ConvLearner.pretrained(arch2, data2, precompute=False)\n",
    "learn2.precompute = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn1.load('1000x1000_256_resnet50_staintools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn2.load('resnet50_new_patches_3000_staintools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveConcatPool2d(\n",
       "    (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (mp): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (9): Flatten()\n",
       "  (10): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Dropout(p=0.25)\n",
       "  (12): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (13): ReLU()\n",
       "  (14): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): Dropout(p=0.5)\n",
       "  (16): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (17): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn2.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Bottom(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ResNet50Bottom, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-8])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "model2_layer9 = ResNet50Bottom(learn2.model)\n",
    "for param in model2_layer9.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model1_layer9 = ResNet50Bottom(learn1.model)\n",
    "for param in model1_layer9.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n"
     ]
    }
   ],
   "source": [
    "# MODEL 2 TRAIN\n",
    "data_model2_trn = pd.DataFrame(columns = list(range(4096)).append('fname'))\n",
    "\n",
    "n = data2.trn_dl.dataset.get_n()\n",
    "\n",
    "for index, name in enumerate(data2.trn_dl.dataset.fnames):\n",
    "    img = data2.trn_dl.get_batch([index])[0]\n",
    "    output = model2_layer9(Variable(V(img)))\n",
    "    \n",
    "    if (index % 100 == 0):\n",
    "        print (index)\n",
    "    \n",
    "    a = output[0].cpu().numpy()\n",
    "    tmp = pd.DataFrame(data=a).T\n",
    "    tmp['fname'] = [name]\n",
    "    data_model2_trn = pd.concat([data_model2_trn, tmp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "# MODEL 2 VAL\n",
    "data_model2_val = pd.DataFrame(columns = list(range(4096)).append('fname'))\n",
    "\n",
    "fnames = data2.val_dl.dataset.fnames\n",
    "\n",
    "for index, name in enumerate(fnames):\n",
    "    img = data2.val_dl.get_batch([index])[0]\n",
    "    output = model2_layer9(Variable(V(img)))\n",
    "    \n",
    "    if (index % 100 == 0):\n",
    "        print (index)\n",
    "    \n",
    "    a = output[0].cpu().numpy()\n",
    "    tmp = pd.DataFrame(data=a).T\n",
    "    tmp['fname'] = [name]\n",
    "    data_model2_val = pd.concat([data_model2_val, tmp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53100\n",
      "53200\n",
      "53300\n",
      "53400\n",
      "53500\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "57000\n",
      "57100\n",
      "57200\n",
      "57300\n",
      "57400\n",
      "57500\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60700\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62500\n",
      "62600\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65100\n",
      "65200\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "68600\n",
      "68700\n",
      "68800\n",
      "68900\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72500\n",
      "72600\n",
      "72700\n",
      "72800\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76200\n",
      "76300\n",
      "76400\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n",
      "78800\n",
      "78900\n",
      "79000\n",
      "79100\n",
      "79200\n",
      "79300\n",
      "79400\n",
      "79500\n",
      "79600\n",
      "79700\n",
      "79800\n",
      "79900\n",
      "80000\n",
      "80100\n",
      "80200\n",
      "80300\n",
      "80400\n",
      "80500\n",
      "80600\n",
      "80700\n",
      "80800\n",
      "80900\n",
      "81000\n",
      "81100\n",
      "81200\n",
      "81300\n",
      "81400\n",
      "81500\n",
      "81600\n",
      "81700\n",
      "81800\n",
      "81900\n",
      "82000\n",
      "82100\n",
      "82200\n",
      "82300\n",
      "82400\n",
      "82500\n",
      "82600\n",
      "82700\n",
      "82800\n",
      "82900\n",
      "83000\n",
      "83100\n",
      "83200\n",
      "83300\n",
      "83400\n",
      "83500\n",
      "83600\n",
      "83700\n",
      "83800\n",
      "83900\n",
      "84000\n",
      "84100\n",
      "84200\n",
      "84300\n",
      "84400\n",
      "84500\n",
      "84600\n",
      "84700\n",
      "84800\n",
      "84900\n",
      "85000\n",
      "85100\n",
      "85200\n",
      "85300\n",
      "85400\n",
      "85500\n",
      "85600\n",
      "85700\n",
      "85800\n",
      "85900\n",
      "86000\n",
      "86100\n",
      "86200\n",
      "86300\n",
      "86400\n",
      "86500\n",
      "86600\n",
      "86700\n",
      "86800\n",
      "86900\n",
      "87000\n",
      "87100\n",
      "87200\n",
      "87300\n",
      "87400\n",
      "87500\n",
      "87600\n",
      "87700\n",
      "87800\n",
      "87900\n",
      "88000\n",
      "88100\n",
      "88200\n",
      "88300\n",
      "88400\n",
      "88500\n",
      "88600\n",
      "88700\n",
      "88800\n",
      "88900\n",
      "89000\n",
      "89100\n",
      "89200\n",
      "89300\n",
      "89400\n",
      "89500\n",
      "89600\n",
      "89700\n",
      "89800\n",
      "89900\n",
      "90000\n",
      "90100\n",
      "90200\n",
      "90300\n",
      "90400\n",
      "90500\n",
      "90600\n",
      "90700\n",
      "90800\n",
      "90900\n",
      "91000\n",
      "91100\n",
      "91200\n",
      "91300\n",
      "91400\n",
      "91500\n",
      "91600\n",
      "91700\n",
      "91800\n",
      "91900\n",
      "92000\n",
      "92100\n",
      "92200\n",
      "92300\n",
      "92400\n",
      "92500\n",
      "92600\n",
      "92700\n",
      "92800\n",
      "92900\n",
      "93000\n",
      "93100\n",
      "93200\n",
      "93300\n",
      "93400\n",
      "93500\n",
      "93600\n",
      "93700\n",
      "93800\n",
      "93900\n",
      "94000\n",
      "94100\n",
      "94200\n",
      "94300\n",
      "94400\n",
      "94500\n",
      "94600\n",
      "94700\n",
      "94800\n",
      "94900\n",
      "95000\n",
      "95100\n",
      "95200\n",
      "95300\n",
      "95400\n",
      "95500\n",
      "95600\n",
      "95700\n",
      "95800\n",
      "95900\n",
      "96000\n",
      "96100\n",
      "96200\n",
      "96300\n",
      "96400\n",
      "96500\n",
      "96600\n",
      "96700\n",
      "96800\n",
      "96900\n",
      "97000\n",
      "97100\n",
      "97200\n",
      "97300\n",
      "97400\n",
      "97500\n",
      "97600\n",
      "97700\n",
      "97800\n",
      "97900\n",
      "98000\n",
      "98100\n",
      "98200\n",
      "98300\n",
      "98400\n",
      "98500\n",
      "98600\n",
      "98700\n",
      "98800\n",
      "98900\n",
      "99000\n",
      "99100\n",
      "99200\n",
      "99300\n",
      "99400\n",
      "99500\n",
      "99600\n",
      "99700\n",
      "99800\n",
      "99900\n",
      "100000\n",
      "100100\n",
      "100200\n",
      "100300\n",
      "100400\n",
      "100500\n",
      "100600\n",
      "100700\n",
      "100800\n",
      "100900\n",
      "101000\n",
      "101100\n",
      "101200\n",
      "101300\n",
      "101400\n",
      "101500\n",
      "101600\n",
      "101700\n",
      "101800\n",
      "101900\n",
      "102000\n",
      "102100\n",
      "102200\n",
      "102300\n",
      "102400\n",
      "102500\n",
      "102600\n",
      "102700\n",
      "102800\n",
      "102900\n",
      "103000\n",
      "103100\n",
      "103200\n",
      "103300\n",
      "103400\n",
      "103500\n",
      "103600\n",
      "103700\n",
      "103800\n",
      "103900\n",
      "104000\n",
      "104100\n",
      "104200\n",
      "104400\n",
      "104500\n",
      "104600\n",
      "104700\n",
      "104800\n",
      "104900\n",
      "105000\n",
      "105100\n",
      "105200\n",
      "105300\n",
      "105400\n",
      "105500\n",
      "105600\n",
      "105700\n",
      "105800\n",
      "105900\n",
      "106000\n",
      "106100\n",
      "106200\n",
      "106300\n",
      "106400\n",
      "106500\n",
      "106600\n",
      "106700\n",
      "106800\n",
      "106900\n",
      "107000\n",
      "107100\n",
      "107200\n",
      "107300\n",
      "107400\n",
      "107500\n",
      "107600\n",
      "107700\n",
      "107800\n",
      "107900\n"
     ]
    }
   ],
   "source": [
    "# MODEL 1 TRAIN\n",
    "data_model1_trn = pd.DataFrame(columns = list(range(4096)).append('fname'))\n",
    "\n",
    "n = data1.trn_dl.dataset.get_n()\n",
    "\n",
    "for index, name in enumerate(data1.trn_dl.dataset.fnames):\n",
    "    img = data1.trn_dl.get_batch([index])[0]\n",
    "    output = model1_layer9(Variable(V(img)))\n",
    "    \n",
    "    if (index % 100 == 0):\n",
    "        print (index)\n",
    "    \n",
    "    a = output[0].cpu().numpy()\n",
    "    tmp = pd.DataFrame(data=a).T\n",
    "    tmp['fname'] = [name]\n",
    "    data_model1_trn = pd.concat([data_model1_trn, tmp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n"
     ]
    }
   ],
   "source": [
    "# MODEL 1 VAL\n",
    "data_model1_val = pd.DataFrame(columns = list(range(4096)).append('fname'))\n",
    "\n",
    "fnames = data1.val_dl.dataset.fnames\n",
    "\n",
    "for index, name in enumerate(fnames):\n",
    "    img = data1.val_dl.get_batch([index])[0]\n",
    "    output = model1_layer9(Variable(V(img)))\n",
    "    \n",
    "    if (index % 100 == 0):\n",
    "        print (index)\n",
    "    \n",
    "    a = output[0].cpu().numpy()\n",
    "    tmp = pd.DataFrame(data=a).T\n",
    "    tmp['fname'] = [name]\n",
    "    data_model1_val = pd.concat([data_model1_val, tmp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model1_trn['target'] = data1.trn_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model1_val['target'] = data1.val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model2_trn['target'] = data2.trn_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model2_val['target'] = data2.val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model2_trn.to_csv('../results/data_3000x3000_train_resnet50_layer9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model2_val.to_csv('../results/data_3000x3000_valid_resnet50_layer9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model1_trn.to_csv('../results/data_1000x1000_train_resnet50_layer9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model1_val.to_csv('../results/data_1000x1000_valid_resnet50_layer9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30165"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data1.val_dl.dataset.fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = iter(data2.trn_dl)\n",
    "inputs, labels = next(i)\n",
    "inputs, labels = Variable(inputs), Variable(labels)\n",
    "outputs = res50_conv2(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9819"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2.trn_dl.dataset.fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4096])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8885, 3.4051, 1.9177,  ..., 0.5425, 0.2665, 0.6057], device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = next(i)\n",
    "inputs, labels = Variable(inputs), Variable(labels)\n",
    "outputs = res50_conv2(inputs)\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = iter(data2.trn_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model2_layer9(Variable(V(data2.trn_dl.get_batch([9818, 1])[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.081404</td>\n",
       "      <td>6.544422</td>\n",
       "      <td>2.773311</td>\n",
       "      <td>3.486383</td>\n",
       "      <td>4.855809</td>\n",
       "      <td>4.829822</td>\n",
       "      <td>4.377133</td>\n",
       "      <td>4.187634</td>\n",
       "      <td>2.618723</td>\n",
       "      <td>2.574072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386802</td>\n",
       "      <td>0.423455</td>\n",
       "      <td>0.41649</td>\n",
       "      <td>0.38185</td>\n",
       "      <td>0.404462</td>\n",
       "      <td>0.192222</td>\n",
       "      <td>0.421217</td>\n",
       "      <td>0.333867</td>\n",
       "      <td>0.452976</td>\n",
       "      <td>0.318715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  2.081404  6.544422  2.773311  3.486383  4.855809  4.829822  4.377133   \n",
       "\n",
       "       7         8         9       ...         4086      4087     4088  \\\n",
       "0  4.187634  2.618723  2.574072    ...     0.386802  0.423455  0.41649   \n",
       "\n",
       "      4089      4090      4091      4092      4093      4094      4095  \n",
       "0  0.38185  0.404462  0.192222  0.421217  0.333867  0.452976  0.318715  \n",
       "\n",
       "[1 rows x 4096 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=a).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 2.20931,  2.2084 ,  2.2315 , ...,  2.21325,  2.21308,  2.19803],\n",
       "         [ 2.20635,  2.22136,  2.213  , ...,  2.20036,  2.21115,  2.20583],\n",
       "         [ 2.21964,  2.2162 ,  2.21311, ...,  2.22099,  2.21844,  2.2132 ],\n",
       "         ...,\n",
       "         [ 2.16112,  2.14096,  1.3836 , ...,  2.18119,  2.20821,  2.2176 ],\n",
       "         [ 2.15107,  2.13198,  2.00224, ...,  2.18312,  2.18173,  2.19945],\n",
       "         [ 1.76234,  2.13101,  2.18604, ...,  2.19154,  2.19191,  2.19156]],\n",
       "\n",
       "        [[ 2.38809,  2.38716,  2.41078, ...,  2.39212,  2.39194,  2.37656],\n",
       "         [ 2.38507,  2.40041,  2.39186, ...,  2.37894,  2.38997,  2.38453],\n",
       "         [ 2.39083,  2.38607,  2.38573, ...,  2.40003,  2.39402,  2.38479],\n",
       "         ...,\n",
       "         [ 2.2071 ,  2.21112,  0.86038, ...,  2.2718 ,  2.30169,  2.31049],\n",
       "         [ 2.22089,  2.131  ,  1.88265, ...,  2.27378,  2.2808 ,  2.29047],\n",
       "         [ 1.57282,  2.17888,  2.21414, ...,  2.28238,  2.3007 ,  2.28241]],\n",
       "\n",
       "        [[ 2.5997 ,  2.59878,  2.62229, ...,  2.60371,  2.60353,  2.58822],\n",
       "         [ 2.59669,  2.61196,  2.60345, ...,  2.59059,  2.60157,  2.59616],\n",
       "         [ 2.61301,  2.61003,  2.60516, ...,  2.61158,  2.60731,  2.60111],\n",
       "         ...,\n",
       "         [ 2.55575,  2.54137,  1.63903, ...,  2.55364,  2.59066,  2.59216],\n",
       "         [ 2.56418,  2.5265 ,  2.34271, ...,  2.55561,  2.56421,  2.57223],\n",
       "         [ 2.09043,  2.5133 ,  2.57571, ...,  2.56418,  2.57646,  2.5642 ]]],\n",
       "\n",
       "\n",
       "       [[[-0.34788, -0.20032,  0.68356, ...,  2.20301,  2.19348,  2.19828],\n",
       "         [-0.18399,  0.04137,  0.75108, ...,  2.19619,  2.18081,  2.18758],\n",
       "         [ 0.16429,  0.2768 ,  0.54282, ...,  2.17736,  2.18626,  2.20029],\n",
       "         ...,\n",
       "         [-0.23115, -0.22996, -0.28407, ...,  2.20357,  2.20888,  2.20181],\n",
       "         [-0.01345,  0.26845,  0.56768, ...,  2.20888,  2.20724,  2.21155],\n",
       "         [ 0.83558,  0.86744,  0.65316, ...,  2.21186,  2.2202 ,  2.22009]],\n",
       "\n",
       "        [[-1.41814, -1.31402, -0.48152, ...,  2.34714,  2.33689,  2.3418 ],\n",
       "         [-1.32626, -1.12326, -0.45477, ...,  2.34017,  2.32394,  2.33086],\n",
       "         [-1.04083, -0.95289, -0.67448, ...,  2.32041,  2.32951,  2.34386],\n",
       "         ...,\n",
       "         [-1.34579, -1.34291, -1.39746, ...,  2.38223,  2.37486,  2.34584],\n",
       "         [-1.21775, -0.9446 , -0.62536, ...,  2.37486,  2.36507,  2.35647],\n",
       "         [-0.41438, -0.45262, -0.6441 , ...,  2.35772,  2.36625,  2.3641 ]],\n",
       "\n",
       "        [[-0.33732, -0.13665,  1.02923, ...,  2.60992,  2.60022,  2.60553],\n",
       "         [-0.13497,  0.20146,  1.16051, ...,  2.60327,  2.58811,  2.59495],\n",
       "         [ 0.32329,  0.56378,  0.93017, ...,  2.58506,  2.59619,  2.59755],\n",
       "         ...,\n",
       "         [-0.13752, -0.14569, -0.2175 , ...,  2.61904,  2.6194 ,  2.60776],\n",
       "         [ 0.1701 ,  0.56668,  0.93648, ...,  2.6234 ,  2.62306,  2.62353],\n",
       "         [ 1.37278,  1.46742,  1.17935, ...,  2.61611,  2.61   ,  2.6162 ]]],\n",
       "\n",
       "\n",
       "       [[[ 1.0902 ,  0.87163,  0.55814, ...,  2.24376,  2.2415 ,  2.24449],\n",
       "         [ 0.67687,  0.78936,  0.72602, ...,  2.24841,  2.24834,  2.24477],\n",
       "         [ 0.49795,  0.74766,  0.76773, ...,  2.24883,  2.24891,  2.24849],\n",
       "         ...,\n",
       "         [ 2.22306,  2.21846,  2.20573, ...,  0.89791,  0.8048 ,  1.05498],\n",
       "         [ 2.21998,  2.21037,  2.21228, ...,  0.89216,  0.88283,  0.84003],\n",
       "         [ 2.21421,  2.21339,  2.20559, ...,  1.24095,  1.23431,  1.16268]],\n",
       "\n",
       "        [[-0.10464, -0.42885, -0.78979, ...,  2.42331,  2.42099,  2.42405],\n",
       "         [-0.65262, -0.5664 , -0.62014, ...,  2.42806,  2.42799,  2.42434],\n",
       "         [-0.85025, -0.60071, -0.57868, ...,  2.42849,  2.42857,  2.42814],\n",
       "         ...,\n",
       "         [ 2.32876,  2.34082,  2.34126, ..., -0.43533, -0.50405, -0.1772 ],\n",
       "         [ 2.33065,  2.32889,  2.35662, ..., -0.37931, -0.44545, -0.45408],\n",
       "         [ 2.35809,  2.33363,  2.34928, ...,  0.18994,  0.17886,  0.02031]],\n",
       "\n",
       "        [[ 1.72065,  1.53917,  1.12583, ...,  2.63476,  2.63246,  2.6355 ],\n",
       "         [ 1.28073,  1.46724,  1.36032, ...,  2.6395 ,  2.63942,  2.63579],\n",
       "         [ 1.01774,  1.38274,  1.42299, ...,  2.63992,  2.64   ,  2.63957],\n",
       "         ...,\n",
       "         [ 2.60565,  2.60841,  2.60452, ...,  1.58059,  1.4795 ,  1.72765],\n",
       "         [ 2.60152,  2.60093,  2.61681, ...,  1.55766,  1.53446,  1.47802],\n",
       "         [ 2.60616,  2.6051 ,  2.61305, ...,  1.86455,  1.8488 ,  1.7511 ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 2.22603,  2.22618,  2.23302, ...,  1.23951,  1.71364,  1.32202],\n",
       "         [ 2.23472,  2.22903,  2.23587, ...,  1.74916,  1.55889,  0.69853],\n",
       "         [ 2.22751,  2.22832,  2.23722, ...,  2.10143,  0.79139,  0.38799],\n",
       "         ...,\n",
       "         [ 2.21655,  2.21879,  2.22418, ...,  0.41459, -0.08474,  1.06546],\n",
       "         [ 2.22553,  2.22351,  2.23492, ...,  1.10158,  0.62208,  1.12698],\n",
       "         [ 2.23315,  2.23081,  2.22642, ...,  1.66553,  1.17412,  1.1751 ]],\n",
       "\n",
       "        [[ 2.40518,  2.40534,  2.41233, ...,  0.26333,  1.12945,  0.35842],\n",
       "         [ 2.41406,  2.40825,  2.41525, ...,  1.29754,  0.82947, -0.60043],\n",
       "         [ 2.4067 ,  2.40752,  2.41663, ...,  1.98661, -0.28464, -0.93588],\n",
       "         ...,\n",
       "         [ 2.3955 ,  2.39779,  2.40329, ..., -0.81258, -1.28902, -0.07752],\n",
       "         [ 2.39603,  2.39117,  2.4023 , ...,  0.0065 , -0.63126,  0.03766],\n",
       "         [ 2.40223,  2.40059,  2.39635, ...,  0.89979,  0.05325,  0.05145]],\n",
       "\n",
       "        [[ 2.61671,  2.61687,  2.62383, ...,  1.7931 ,  2.19505,  1.88175],\n",
       "         [ 2.62556,  2.61976,  2.62673, ...,  2.11968,  2.10098,  1.29848],\n",
       "         [ 2.61822,  2.61904,  2.62811, ...,  2.54428,  1.31857,  0.86225],\n",
       "         ...,\n",
       "         [ 2.60707,  2.60935,  2.61483, ...,  0.82993,  0.10283,  1.61315],\n",
       "         [ 2.61664,  2.61901,  2.6289 , ...,  1.60004,  1.04606,  1.67725],\n",
       "         [ 2.62643,  2.62456,  2.61779, ...,  2.21367,  1.77968,  1.79492]]],\n",
       "\n",
       "\n",
       "       [[[ 2.24891,  2.24891,  2.24691, ...,  2.24317,  2.24684,  2.24841],\n",
       "         [ 2.24891,  2.24891,  2.24327, ...,  2.24245,  2.24777,  2.24043],\n",
       "         [ 2.24274,  2.24558,  2.24655, ...,  2.24384,  2.24855,  2.24691],\n",
       "         ...,\n",
       "         [ 2.22674,  2.22826,  2.24119, ...,  2.22153,  2.22233,  2.24655],\n",
       "         [ 2.24185,  2.24033,  2.24261, ...,  2.22568,  2.23242,  2.2301 ],\n",
       "         [ 2.24891,  2.23779,  2.2426 , ...,  2.22564,  2.22985,  2.23157]],\n",
       "\n",
       "        [[ 2.42857,  2.42857,  2.42653, ...,  2.42271,  2.42645,  2.42806],\n",
       "         [ 2.42857,  2.42857,  2.4228 , ...,  2.42197,  2.42741,  2.4199 ],\n",
       "         [ 2.42226,  2.42517,  2.42616, ...,  2.42339,  2.42821,  2.42652],\n",
       "         ...,\n",
       "         [ 2.4059 ,  2.40746,  2.42068, ...,  2.40058,  2.4014 ,  2.42616],\n",
       "         [ 2.42135,  2.4198 ,  2.42214, ...,  2.40483,  2.41171,  2.40935],\n",
       "         [ 2.42857,  2.4172 ,  2.42212, ...,  2.40479,  2.40603,  2.41085]],\n",
       "\n",
       "        [[ 2.64   ,  2.64   ,  2.63797, ...,  2.63416,  2.63789,  2.63949],\n",
       "         [ 2.64   ,  2.64   ,  2.63426, ...,  2.63343,  2.63884,  2.63137],\n",
       "         [ 2.63372,  2.63661,  2.6376 , ...,  2.63484,  2.63964,  2.63796],\n",
       "         ...,\n",
       "         [ 2.61743,  2.61898,  2.63215, ...,  2.61214,  2.61295,  2.6376 ],\n",
       "         [ 2.63281,  2.63127,  2.63359, ...,  2.61636,  2.62322,  2.62086],\n",
       "         [ 2.64   ,  2.62868,  2.63358, ...,  2.61632,  2.61959,  2.62236]]],\n",
       "\n",
       "\n",
       "       [[[ 2.24157,  2.23578,  2.23581, ...,  2.23835,  2.23621,  2.24891],\n",
       "         [ 2.23607,  2.24271,  2.24363, ...,  2.23368,  2.23566,  2.23832],\n",
       "         [ 2.23457,  2.23985,  2.23659, ...,  2.24207,  2.24576,  2.23554],\n",
       "         ...,\n",
       "         [ 0.06049,  0.38037,  0.4568 , ...,  1.59115,  1.11838,  1.48812],\n",
       "         [ 0.38428, -0.25997,  0.07947, ...,  1.15659,  0.90532,  0.91715],\n",
       "         [ 0.48972, -0.26483, -0.32464, ...,  0.31556,  0.34468, -0.13432]],\n",
       "\n",
       "        [[ 2.42107,  2.41515,  2.41518, ...,  2.41778,  2.41559,  2.42857],\n",
       "         [ 2.41545,  2.42224,  2.42318, ...,  2.41266,  2.4048 ,  2.41775],\n",
       "         [ 2.41391,  2.41931,  2.41598, ...,  2.42063,  2.41341,  2.4149 ],\n",
       "         ...,\n",
       "         [-1.09084, -0.84016, -0.81885, ...,  0.92614,  0.14455,  0.81969],\n",
       "         [-0.72714, -1.40388, -1.08269, ...,  0.23094, -0.27564, -0.07564],\n",
       "         [-0.67817, -1.34912, -1.41456, ..., -0.86428, -0.83375, -1.21866]],\n",
       "\n",
       "        [[ 2.63253,  2.62664,  2.62667, ...,  2.62926,  2.63346,  2.64   ],\n",
       "         [ 2.62694,  2.63369,  2.63463, ...,  2.6244 ,  2.62951,  2.62922],\n",
       "         [ 2.62541,  2.63078,  2.62747, ...,  2.63271,  2.63545,  2.62639],\n",
       "         ...,\n",
       "         [ 0.1578 ,  0.63598,  0.85129, ...,  2.0252 ,  1.49862,  1.82989],\n",
       "         [ 0.5827 , -0.15408,  0.30321, ...,  1.54671,  1.40275,  1.23994],\n",
       "         [ 0.86727, -0.27411, -0.32408, ...,  0.51363,  0.58566, -0.08672]]]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.trn_dl.get_batch(list(range(0, 63)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.65136,  1.59942,  1.43624, ...,  0.93642,  1.17164,  0.75017],\n",
       "        [ 1.38131,  1.41287,  1.16173, ...,  0.94259,  0.8877 ,  0.11419],\n",
       "        [ 1.2332 ,  1.11449,  1.39986, ...,  0.73916,  0.68656,  0.19979],\n",
       "        ...,\n",
       "        [ 2.23416,  2.23951,  2.22591, ...,  2.23541,  2.23908,  2.22926],\n",
       "        [ 2.22168,  2.23207,  2.22627, ...,  2.23016,  2.23426,  2.22651],\n",
       "        [ 2.22473,  2.22475,  2.23496, ...,  2.23063,  2.22432,  2.23869]],\n",
       "\n",
       "       [[ 0.9711 ,  0.93752,  0.5783 , ..., -0.31907,  0.08526, -0.34831],\n",
       "        [ 0.45204,  0.40131,  0.08633, ..., -0.32827, -0.38344, -1.0391 ],\n",
       "        [ 0.16897,  0.02024,  0.54729, ..., -0.57599, -0.60046, -1.01487],\n",
       "        ...,\n",
       "        [ 2.41349,  2.41896,  2.40506, ...,  2.41477,  2.41852,  2.40848],\n",
       "        [ 2.40074,  2.41136,  2.40279, ...,  2.4094 ,  2.41046,  2.39499],\n",
       "        [ 2.40386,  2.40387,  2.40569, ...,  2.40989,  2.39188,  2.38945]],\n",
       "\n",
       "       [[ 2.14987,  2.10124,  1.96512, ...,  1.58498,  1.80771,  1.10032],\n",
       "        [ 1.92498,  1.99289,  1.72249, ...,  1.57641,  1.47394,  0.37272],\n",
       "        [ 1.83021,  1.64149,  1.90784, ...,  1.28897,  1.18914,  0.44842],\n",
       "        ...,\n",
       "        [ 2.62499,  2.63043,  2.6166 , ...,  2.62626,  2.63   ,  2.62   ],\n",
       "        [ 2.61229,  2.62287,  2.61758, ...,  2.62091,  2.62525,  2.62089],\n",
       "        [ 2.61539,  2.61541,  2.62911, ...,  2.6214 ,  2.61816,  2.63679]]], dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data2.trn_dl.dataset.get1item(12)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.65136,  1.59942,  1.43624, ...,  0.93642,  1.17164,  0.75017],\n",
       "         [ 1.38131,  1.41287,  1.16173, ...,  0.94259,  0.8877 ,  0.11419],\n",
       "         [ 1.2332 ,  1.11449,  1.39986, ...,  0.73916,  0.68656,  0.19979],\n",
       "         ...,\n",
       "         [ 2.23416,  2.23951,  2.22591, ...,  2.23541,  2.23908,  2.22926],\n",
       "         [ 2.22168,  2.23207,  2.22627, ...,  2.23016,  2.23426,  2.22651],\n",
       "         [ 2.22473,  2.22475,  2.23496, ...,  2.23063,  2.22432,  2.23869]],\n",
       "\n",
       "        [[ 0.9711 ,  0.93752,  0.5783 , ..., -0.31907,  0.08526, -0.34831],\n",
       "         [ 0.45204,  0.40131,  0.08633, ..., -0.32827, -0.38344, -1.0391 ],\n",
       "         [ 0.16897,  0.02024,  0.54729, ..., -0.57599, -0.60046, -1.01487],\n",
       "         ...,\n",
       "         [ 2.41349,  2.41896,  2.40506, ...,  2.41477,  2.41852,  2.40848],\n",
       "         [ 2.40074,  2.41136,  2.40279, ...,  2.4094 ,  2.41046,  2.39499],\n",
       "         [ 2.40386,  2.40387,  2.40569, ...,  2.40989,  2.39188,  2.38945]],\n",
       "\n",
       "        [[ 2.14987,  2.10124,  1.96512, ...,  1.58498,  1.80771,  1.10032],\n",
       "         [ 1.92498,  1.99289,  1.72249, ...,  1.57641,  1.47394,  0.37272],\n",
       "         [ 1.83021,  1.64149,  1.90784, ...,  1.28897,  1.18914,  0.44842],\n",
       "         ...,\n",
       "         [ 2.62499,  2.63043,  2.6166 , ...,  2.62626,  2.63   ,  2.62   ],\n",
       "         [ 2.61229,  2.62287,  2.61758, ...,  2.62091,  2.62525,  2.62089],\n",
       "         [ 2.61539,  2.61541,  2.62911, ...,  2.6214 ,  2.61816,  2.63679]]]], dtype=float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.trn_dl.get_batch([12])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
